{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/07/2021 11:04:31 AM adding document #0 to Dictionary(0 unique tokens: [])\n",
      "12/07/2021 11:04:31 AM built Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...) from 9 documents (total 29 corpus positions)\n",
      "12/07/2021 11:04:31 AM Dictionary lifecycle event {'msg': \"built Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...) from 9 documents (total 29 corpus positions)\", 'datetime': '2021-12-07T11:04:31.195503', 'gensim': '4.1.2', 'python': '3.6.13 |Anaconda, Inc.| (default, Mar 16 2021, 11:37:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "import httrees.embeddings as emb\n",
    "import pandas as pd\n",
    "from gensim.utils import effective_n_jobs\n",
    "from gensim.models import Word2Vec, fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning embeddings: `httrees.embeddings`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we'll walk through the process of fine-tuning some pre-trained word2vec embeddings from `gensim` on our own data.\n",
    "\n",
    "We'll use the Amazon reviews data again ([see the previous documentation](https://github.com/bllguo/CourseProject/blob/main/docs.ipynb)), but only the actual reviews text. I prepared a separate `.csv` file with only the reviews: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The description and photo on this product need...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This was a great book!!!! It is well thought t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am a first year teacher, teaching 5th grade....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I got the book at my bookfair at school lookin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi! I'm Martine Redman and I created this puzz...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text\n",
       "0  The description and photo on this product need...\n",
       "1  This was a great book!!!! It is well thought t...\n",
       "2  I am a first year teacher, teaching 5th grade....\n",
       "3  I got the book at my bookfair at school lookin...\n",
       "4  Hi! I'm Martine Redman and I created this puzz..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/docs.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, this can be passed to the model for training already, but `httrees.embeddings` also provides a `StreamCorpus` class that can be used to stream your text to your model instead of loading the entire corpus into memory at once. This is useful for larger datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'description', 'and', 'photo', 'on', 'this', 'product', 'needs', 'to', 'be', 'changed', 'to', 'indicate', 'this', 'product', 'is', 'the', 'buffalos', 'version', 'of', 'this', 'beef', 'jerky.']\n"
     ]
    }
   ],
   "source": [
    "corpus = emb.StreamCorpus('data/docs.csv', encoding='utf-8-sig', skip_rows=1)\n",
    "for doc in corpus:\n",
    "    print(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two wrappers are provided around `gensim` embedding models, `EmbeddingTuner` and the more specific `Word2VecTuner`.\n",
    "`EmbeddingTuner` is minimalistic. As an example, we'll wrap a `FastText` model and do some additional training. We can obtain pretrained `FastText` models [from Facebook](https://fasttext.cc/docs/en/crawl-vectors.html).\n",
    "\n",
    "Note: more extensive examples are available from `gensim`'s [documentation](https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.load_facebook_model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/07/2021 11:04:31 AM loading 2000000 words for fastText model from cc.en.300.bin\n",
      "12/07/2021 11:04:51 AM FastText lifecycle event {'params': 'FastText(vocab=0, vector_size=300, alpha=0.025)', 'datetime': '2021-12-07T11:04:51.968603', 'gensim': '4.1.2', 'python': '3.6.13 |Anaconda, Inc.| (default, Mar 16 2021, 11:37:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'created'}\n",
      "12/07/2021 11:04:51 AM Updating model with new vocabulary\n",
      "12/07/2021 11:05:00 AM FastText lifecycle event {'msg': 'added 2000000 new unique words (100.0%% of original 2000000) and increased the count of 0 pre-existing words (0.0%% of original 2000000)', 'datetime': '2021-12-07T11:05:00.863008', 'gensim': '4.1.2', 'python': '3.6.13 |Anaconda, Inc.| (default, Mar 16 2021, 11:37:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "12/07/2021 11:05:10 AM deleting the raw counts dictionary of 2000000 items\n",
      "12/07/2021 11:05:10 AM sample=1e-05 downsamples 3498 most-common words\n",
      "12/07/2021 11:05:10 AM FastText lifecycle event {'msg': 'downsampling leaves estimated 195157728967.50262 word corpus (35.4%% of prior 552001338161)', 'datetime': '2021-12-07T11:05:10.616083', 'gensim': '4.1.2', 'python': '3.6.13 |Anaconda, Inc.| (default, Mar 16 2021, 11:37:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "12/07/2021 11:06:08 AM FastText lifecycle event {'msg': 'loaded (4000000, 300) weight matrix for fastText model from cc.en.300.bin', 'datetime': '2021-12-07T11:06:08.291820', 'gensim': '4.1.2', 'python': '3.6.13 |Anaconda, Inc.| (default, Mar 16 2021, 11:37:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'load_fasttext_format'}\n"
     ]
    }
   ],
   "source": [
    "model = fasttext.load_facebook_model('cc.en.300.bin')\n",
    "tuner = emb.EmbeddingTuner(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we simply update the vocabulary, then train on top with our own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/07/2021 11:06:08 AM collecting all words and their counts\n",
      "12/07/2021 11:06:08 AM PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "12/07/2021 11:06:09 AM PROGRESS: at sentence #10000, processed 900659 words, keeping 59826 word types\n",
      "12/07/2021 11:06:09 AM PROGRESS: at sentence #20000, processed 1679397 words, keeping 90519 word types\n",
      "12/07/2021 11:06:09 AM PROGRESS: at sentence #30000, processed 2490190 words, keeping 116946 word types\n",
      "12/07/2021 11:06:09 AM collected 140947 word types from a corpus of 3294344 raw words and 40000 sentences\n",
      "12/07/2021 11:06:09 AM Updating model with new vocabulary\n",
      "12/07/2021 11:06:16 AM FastText lifecycle event {'msg': 'added 6106 new unique words (4.332124841252385%% of original 140947) and increased the count of 16640 pre-existing words (11.805856101939026%% of original 140947)', 'datetime': '2021-12-07T11:06:16.117852', 'gensim': '4.1.2', 'python': '3.6.13 |Anaconda, Inc.| (default, Mar 16 2021, 11:37:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "12/07/2021 11:06:16 AM deleting the raw counts dictionary of 140947 items\n",
      "12/07/2021 11:06:16 AM sample=1e-05 downsamples 2872 most-common words\n",
      "12/07/2021 11:06:16 AM FastText lifecycle event {'msg': 'downsampling leaves estimated 770779.5578716721 word corpus (24.6%% of prior 3134262)', 'datetime': '2021-12-07T11:06:16.242889', 'gensim': '4.1.2', 'python': '3.6.13 |Anaconda, Inc.| (default, Mar 16 2021, 11:37:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "12/07/2021 11:06:34 AM estimated required memory for 2006106 words, 2000000 buckets and 300 dimensions: 8470777260 bytes\n",
      "12/07/2021 11:06:34 AM updating layer weights\n",
      "12/07/2021 11:07:12 AM FastText lifecycle event {'update': True, 'trim_rule': 'None', 'datetime': '2021-12-07T11:07:12.492670', 'gensim': '4.1.2', 'python': '3.6.13 |Anaconda, Inc.| (default, Mar 16 2021, 11:37:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'build_vocab'}\n",
      "12/07/2021 11:07:12 AM FastText lifecycle event {'msg': 'training model with 3 workers on 2006106 vocabulary and 300 features, using sg=0 hs=0 sample=9.999999747378752e-06 negative=10 window=5 shrink_windows=True', 'datetime': '2021-12-07T11:07:12.533670', 'gensim': '4.1.2', 'python': '3.6.13 |Anaconda, Inc.| (default, Mar 16 2021, 11:37:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'train'}\n",
      "12/07/2021 11:07:13 AM EPOCH 1 - PROGRESS: at 35.58% examples, 302241 words/s, in_qsize 6, out_qsize 0\n",
      "12/07/2021 11:07:14 AM EPOCH 1 - PROGRESS: at 77.18% examples, 318075 words/s, in_qsize 4, out_qsize 0\n",
      "12/07/2021 11:07:15 AM worker thread finished; awaiting finish of 2 more threads\n",
      "12/07/2021 11:07:15 AM worker thread finished; awaiting finish of 1 more threads\n",
      "12/07/2021 11:07:15 AM worker thread finished; awaiting finish of 0 more threads\n",
      "12/07/2021 11:07:15 AM EPOCH - 1 : training on 3294344 raw words (821768 effective words) took 2.6s, 321709 effective words/s\n",
      "12/07/2021 11:07:15 AM EPOCH - 1 : supplied example count (40000) did not equal expected count (40001)\n",
      "12/07/2021 11:07:15 AM FastText lifecycle event {'msg': 'training on 3294344 raw words (821768 effective words) took 2.6s, 321271 effective words/s', 'datetime': '2021-12-07T11:07:15.091860', 'gensim': '4.1.2', 'python': '3.6.13 |Anaconda, Inc.| (default, Mar 16 2021, 11:37:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'train'}\n"
     ]
    }
   ],
   "source": [
    "tuner.train(corpus, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes the model itself is not available. For example, for `word2vec`, from `gensim` we only have the `KeyedVectors`. We don't have the internal weights of the model, but we can still train with these vectors as a starting point.\n",
    "\n",
    "We can load in embeddings to the `Word2VecTuner` either from `KeyedVectors` or directly from `gensim`'s data repository; in this case we'll use the latter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/07/2021 11:07:41 AM Word2Vec lifecycle event {'params': 'Word2Vec(vocab=0, vector_size=300, alpha=0.025)', 'datetime': '2021-12-07T11:07:41.099668', 'gensim': '4.1.2', 'python': '3.6.13 |Anaconda, Inc.| (default, Mar 16 2021, 11:37:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "model = emb.Word2VecTuner(Word2Vec(window=3, \n",
    "                                   min_count=1, \n",
    "                                   workers=effective_n_jobs(-1)-1, \n",
    "                                   vector_size=300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/07/2021 11:07:41 AM Downloading word2vec-google-news-300...\n",
      "12/07/2021 11:07:42 AM loading projection weights from C:\\Users\\bllgu/gensim-data\\word2vec-google-news-300\\word2vec-google-news-300.gz\n",
      "12/07/2021 11:08:20 AM KeyedVectors lifecycle event {'msg': 'loaded (3000000, 300) matrix of type float32 from C:\\\\Users\\\\bllgu/gensim-data\\\\word2vec-google-news-300\\\\word2vec-google-news-300.gz', 'binary': True, 'encoding': 'utf8', 'datetime': '2021-12-07T11:08:20.547289', 'gensim': '4.1.2', 'python': '3.6.13 |Anaconda, Inc.| (default, Mar 16 2021, 11:37:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'load_word2vec_format'}\n",
      "12/07/2021 11:08:20 AM Download complete.\n",
      "12/07/2021 11:08:20 AM Updating model vocabulary...\n",
      "12/07/2021 11:08:22 AM collecting all words and their counts\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #10000, processed 10000 words, keeping 10000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #20000, processed 20000 words, keeping 20000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #30000, processed 30000 words, keeping 30000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #40000, processed 40000 words, keeping 40000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #50000, processed 50000 words, keeping 50000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #60000, processed 60000 words, keeping 60000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #70000, processed 70000 words, keeping 70000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #80000, processed 80000 words, keeping 80000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #90000, processed 90000 words, keeping 90000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #100000, processed 100000 words, keeping 100000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #110000, processed 110000 words, keeping 110000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #120000, processed 120000 words, keeping 120000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #130000, processed 130000 words, keeping 130000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #140000, processed 140000 words, keeping 140000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #150000, processed 150000 words, keeping 150000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #160000, processed 160000 words, keeping 160000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #170000, processed 170000 words, keeping 170000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #180000, processed 180000 words, keeping 180000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #190000, processed 190000 words, keeping 190000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #200000, processed 200000 words, keeping 200000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #210000, processed 210000 words, keeping 210000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #220000, processed 220000 words, keeping 220000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #230000, processed 230000 words, keeping 230000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #240000, processed 240000 words, keeping 240000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #250000, processed 250000 words, keeping 250000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #260000, processed 260000 words, keeping 260000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #270000, processed 270000 words, keeping 270000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #280000, processed 280000 words, keeping 280000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #290000, processed 290000 words, keeping 290000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #300000, processed 300000 words, keeping 300000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #310000, processed 310000 words, keeping 310000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #320000, processed 320000 words, keeping 320000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #330000, processed 330000 words, keeping 330000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #340000, processed 340000 words, keeping 340000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #350000, processed 350000 words, keeping 350000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #360000, processed 360000 words, keeping 360000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #370000, processed 370000 words, keeping 370000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #380000, processed 380000 words, keeping 380000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #390000, processed 390000 words, keeping 390000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #400000, processed 400000 words, keeping 400000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #410000, processed 410000 words, keeping 410000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #420000, processed 420000 words, keeping 420000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #430000, processed 430000 words, keeping 430000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #440000, processed 440000 words, keeping 440000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #450000, processed 450000 words, keeping 450000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #460000, processed 460000 words, keeping 460000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #470000, processed 470000 words, keeping 470000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #480000, processed 480000 words, keeping 480000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #490000, processed 490000 words, keeping 490000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #500000, processed 500000 words, keeping 500000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #510000, processed 510000 words, keeping 510000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #520000, processed 520000 words, keeping 520000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #530000, processed 530000 words, keeping 530000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #540000, processed 540000 words, keeping 540000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #550000, processed 550000 words, keeping 550000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #560000, processed 560000 words, keeping 560000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #570000, processed 570000 words, keeping 570000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #580000, processed 580000 words, keeping 580000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #590000, processed 590000 words, keeping 590000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #600000, processed 600000 words, keeping 600000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #610000, processed 610000 words, keeping 610000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #620000, processed 620000 words, keeping 620000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #630000, processed 630000 words, keeping 630000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #640000, processed 640000 words, keeping 640000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #650000, processed 650000 words, keeping 650000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #660000, processed 660000 words, keeping 660000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #670000, processed 670000 words, keeping 670000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #680000, processed 680000 words, keeping 680000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #690000, processed 690000 words, keeping 690000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #700000, processed 700000 words, keeping 700000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #710000, processed 710000 words, keeping 710000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #720000, processed 720000 words, keeping 720000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #730000, processed 730000 words, keeping 730000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #740000, processed 740000 words, keeping 740000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #750000, processed 750000 words, keeping 750000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #760000, processed 760000 words, keeping 760000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #770000, processed 770000 words, keeping 770000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #780000, processed 780000 words, keeping 780000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #790000, processed 790000 words, keeping 790000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #800000, processed 800000 words, keeping 800000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #810000, processed 810000 words, keeping 810000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #820000, processed 820000 words, keeping 820000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #830000, processed 830000 words, keeping 830000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #840000, processed 840000 words, keeping 840000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #850000, processed 850000 words, keeping 850000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #860000, processed 860000 words, keeping 860000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #870000, processed 870000 words, keeping 870000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #880000, processed 880000 words, keeping 880000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #890000, processed 890000 words, keeping 890000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #900000, processed 900000 words, keeping 900000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #910000, processed 910000 words, keeping 910000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #920000, processed 920000 words, keeping 920000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #930000, processed 930000 words, keeping 930000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #940000, processed 940000 words, keeping 940000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #950000, processed 950000 words, keeping 950000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #960000, processed 960000 words, keeping 960000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #970000, processed 970000 words, keeping 970000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #980000, processed 980000 words, keeping 980000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #990000, processed 990000 words, keeping 990000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #1000000, processed 1000000 words, keeping 1000000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #1010000, processed 1010000 words, keeping 1010000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #1020000, processed 1020000 words, keeping 1020000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #1030000, processed 1030000 words, keeping 1030000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #1040000, processed 1040000 words, keeping 1040000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #1050000, processed 1050000 words, keeping 1050000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #1060000, processed 1060000 words, keeping 1060000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #1070000, processed 1070000 words, keeping 1070000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #1080000, processed 1080000 words, keeping 1080000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #1090000, processed 1090000 words, keeping 1090000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #1100000, processed 1100000 words, keeping 1100000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #1110000, processed 1110000 words, keeping 1110000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #1120000, processed 1120000 words, keeping 1120000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #1130000, processed 1130000 words, keeping 1130000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #1140000, processed 1140000 words, keeping 1140000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #1150000, processed 1150000 words, keeping 1150000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #1160000, processed 1160000 words, keeping 1160000 word types\n",
      "12/07/2021 11:08:22 AM PROGRESS: at sentence #1170000, processed 1170000 words, keeping 1170000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1180000, processed 1180000 words, keeping 1180000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1190000, processed 1190000 words, keeping 1190000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1200000, processed 1200000 words, keeping 1200000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1210000, processed 1210000 words, keeping 1210000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1220000, processed 1220000 words, keeping 1220000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1230000, processed 1230000 words, keeping 1230000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1240000, processed 1240000 words, keeping 1240000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1250000, processed 1250000 words, keeping 1250000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1260000, processed 1260000 words, keeping 1260000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1270000, processed 1270000 words, keeping 1270000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1280000, processed 1280000 words, keeping 1280000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1290000, processed 1290000 words, keeping 1290000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1300000, processed 1300000 words, keeping 1300000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1310000, processed 1310000 words, keeping 1310000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1320000, processed 1320000 words, keeping 1320000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1330000, processed 1330000 words, keeping 1330000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1340000, processed 1340000 words, keeping 1340000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1350000, processed 1350000 words, keeping 1350000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1360000, processed 1360000 words, keeping 1360000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1370000, processed 1370000 words, keeping 1370000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1380000, processed 1380000 words, keeping 1380000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1390000, processed 1390000 words, keeping 1390000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1400000, processed 1400000 words, keeping 1400000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1410000, processed 1410000 words, keeping 1410000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1420000, processed 1420000 words, keeping 1420000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1430000, processed 1430000 words, keeping 1430000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1440000, processed 1440000 words, keeping 1440000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1450000, processed 1450000 words, keeping 1450000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1460000, processed 1460000 words, keeping 1460000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1470000, processed 1470000 words, keeping 1470000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1480000, processed 1480000 words, keeping 1480000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1490000, processed 1490000 words, keeping 1490000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1500000, processed 1500000 words, keeping 1500000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1510000, processed 1510000 words, keeping 1510000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1520000, processed 1520000 words, keeping 1520000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1530000, processed 1530000 words, keeping 1530000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1540000, processed 1540000 words, keeping 1540000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1550000, processed 1550000 words, keeping 1550000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1560000, processed 1560000 words, keeping 1560000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1570000, processed 1570000 words, keeping 1570000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1580000, processed 1580000 words, keeping 1580000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1590000, processed 1590000 words, keeping 1590000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1600000, processed 1600000 words, keeping 1600000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1610000, processed 1610000 words, keeping 1610000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1620000, processed 1620000 words, keeping 1620000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1630000, processed 1630000 words, keeping 1630000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1640000, processed 1640000 words, keeping 1640000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1650000, processed 1650000 words, keeping 1650000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1660000, processed 1660000 words, keeping 1660000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1670000, processed 1670000 words, keeping 1670000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1680000, processed 1680000 words, keeping 1680000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1690000, processed 1690000 words, keeping 1690000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1700000, processed 1700000 words, keeping 1700000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1710000, processed 1710000 words, keeping 1710000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1720000, processed 1720000 words, keeping 1720000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1730000, processed 1730000 words, keeping 1730000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1740000, processed 1740000 words, keeping 1740000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1750000, processed 1750000 words, keeping 1750000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1760000, processed 1760000 words, keeping 1760000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1770000, processed 1770000 words, keeping 1770000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1780000, processed 1780000 words, keeping 1780000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1790000, processed 1790000 words, keeping 1790000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1800000, processed 1800000 words, keeping 1800000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1810000, processed 1810000 words, keeping 1810000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1820000, processed 1820000 words, keeping 1820000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1830000, processed 1830000 words, keeping 1830000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1840000, processed 1840000 words, keeping 1840000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1850000, processed 1850000 words, keeping 1850000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1860000, processed 1860000 words, keeping 1860000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1870000, processed 1870000 words, keeping 1870000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1880000, processed 1880000 words, keeping 1880000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1890000, processed 1890000 words, keeping 1890000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1900000, processed 1900000 words, keeping 1900000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1910000, processed 1910000 words, keeping 1910000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1920000, processed 1920000 words, keeping 1920000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1930000, processed 1930000 words, keeping 1930000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1940000, processed 1940000 words, keeping 1940000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1950000, processed 1950000 words, keeping 1950000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1960000, processed 1960000 words, keeping 1960000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1970000, processed 1970000 words, keeping 1970000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1980000, processed 1980000 words, keeping 1980000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #1990000, processed 1990000 words, keeping 1990000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2000000, processed 2000000 words, keeping 2000000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2010000, processed 2010000 words, keeping 2010000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2020000, processed 2020000 words, keeping 2020000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2030000, processed 2030000 words, keeping 2030000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2040000, processed 2040000 words, keeping 2040000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2050000, processed 2050000 words, keeping 2050000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2060000, processed 2060000 words, keeping 2060000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2070000, processed 2070000 words, keeping 2070000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2080000, processed 2080000 words, keeping 2080000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2090000, processed 2090000 words, keeping 2090000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2100000, processed 2100000 words, keeping 2100000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2110000, processed 2110000 words, keeping 2110000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2120000, processed 2120000 words, keeping 2120000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2130000, processed 2130000 words, keeping 2130000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2140000, processed 2140000 words, keeping 2140000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2150000, processed 2150000 words, keeping 2150000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2160000, processed 2160000 words, keeping 2160000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2170000, processed 2170000 words, keeping 2170000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2180000, processed 2180000 words, keeping 2180000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2190000, processed 2190000 words, keeping 2190000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2200000, processed 2200000 words, keeping 2200000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2210000, processed 2210000 words, keeping 2210000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2220000, processed 2220000 words, keeping 2220000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2230000, processed 2230000 words, keeping 2230000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2240000, processed 2240000 words, keeping 2240000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2250000, processed 2250000 words, keeping 2250000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2260000, processed 2260000 words, keeping 2260000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2270000, processed 2270000 words, keeping 2270000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2280000, processed 2280000 words, keeping 2280000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2290000, processed 2290000 words, keeping 2290000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2300000, processed 2300000 words, keeping 2300000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2310000, processed 2310000 words, keeping 2310000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2320000, processed 2320000 words, keeping 2320000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2330000, processed 2330000 words, keeping 2330000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2340000, processed 2340000 words, keeping 2340000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2350000, processed 2350000 words, keeping 2350000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2360000, processed 2360000 words, keeping 2360000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2370000, processed 2370000 words, keeping 2370000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2380000, processed 2380000 words, keeping 2380000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2390000, processed 2390000 words, keeping 2390000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2400000, processed 2400000 words, keeping 2400000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2410000, processed 2410000 words, keeping 2410000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2420000, processed 2420000 words, keeping 2420000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2430000, processed 2430000 words, keeping 2430000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2440000, processed 2440000 words, keeping 2440000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2450000, processed 2450000 words, keeping 2450000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2460000, processed 2460000 words, keeping 2460000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2470000, processed 2470000 words, keeping 2470000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2480000, processed 2480000 words, keeping 2480000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2490000, processed 2490000 words, keeping 2490000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2500000, processed 2500000 words, keeping 2500000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2510000, processed 2510000 words, keeping 2510000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2520000, processed 2520000 words, keeping 2520000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2530000, processed 2530000 words, keeping 2530000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2540000, processed 2540000 words, keeping 2540000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2550000, processed 2550000 words, keeping 2550000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2560000, processed 2560000 words, keeping 2560000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2570000, processed 2570000 words, keeping 2570000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2580000, processed 2580000 words, keeping 2580000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2590000, processed 2590000 words, keeping 2590000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2600000, processed 2600000 words, keeping 2600000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2610000, processed 2610000 words, keeping 2610000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2620000, processed 2620000 words, keeping 2620000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2630000, processed 2630000 words, keeping 2630000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2640000, processed 2640000 words, keeping 2640000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2650000, processed 2650000 words, keeping 2650000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2660000, processed 2660000 words, keeping 2660000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2670000, processed 2670000 words, keeping 2670000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2680000, processed 2680000 words, keeping 2680000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2690000, processed 2690000 words, keeping 2690000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2700000, processed 2700000 words, keeping 2700000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2710000, processed 2710000 words, keeping 2710000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2720000, processed 2720000 words, keeping 2720000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2730000, processed 2730000 words, keeping 2730000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2740000, processed 2740000 words, keeping 2740000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2750000, processed 2750000 words, keeping 2750000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2760000, processed 2760000 words, keeping 2760000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2770000, processed 2770000 words, keeping 2770000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2780000, processed 2780000 words, keeping 2780000 word types\n",
      "12/07/2021 11:08:23 AM PROGRESS: at sentence #2790000, processed 2790000 words, keeping 2790000 word types\n",
      "12/07/2021 11:08:24 AM PROGRESS: at sentence #2800000, processed 2800000 words, keeping 2800000 word types\n",
      "12/07/2021 11:08:24 AM PROGRESS: at sentence #2810000, processed 2810000 words, keeping 2810000 word types\n",
      "12/07/2021 11:08:24 AM PROGRESS: at sentence #2820000, processed 2820000 words, keeping 2820000 word types\n",
      "12/07/2021 11:08:24 AM PROGRESS: at sentence #2830000, processed 2830000 words, keeping 2830000 word types\n",
      "12/07/2021 11:08:24 AM PROGRESS: at sentence #2840000, processed 2840000 words, keeping 2840000 word types\n",
      "12/07/2021 11:08:24 AM PROGRESS: at sentence #2850000, processed 2850000 words, keeping 2850000 word types\n",
      "12/07/2021 11:08:24 AM PROGRESS: at sentence #2860000, processed 2860000 words, keeping 2860000 word types\n",
      "12/07/2021 11:08:24 AM PROGRESS: at sentence #2870000, processed 2870000 words, keeping 2870000 word types\n",
      "12/07/2021 11:08:24 AM PROGRESS: at sentence #2880000, processed 2880000 words, keeping 2880000 word types\n",
      "12/07/2021 11:08:24 AM PROGRESS: at sentence #2890000, processed 2890000 words, keeping 2890000 word types\n",
      "12/07/2021 11:08:24 AM PROGRESS: at sentence #2900000, processed 2900000 words, keeping 2900000 word types\n",
      "12/07/2021 11:08:24 AM PROGRESS: at sentence #2910000, processed 2910000 words, keeping 2910000 word types\n",
      "12/07/2021 11:08:24 AM PROGRESS: at sentence #2920000, processed 2920000 words, keeping 2920000 word types\n",
      "12/07/2021 11:08:24 AM PROGRESS: at sentence #2930000, processed 2930000 words, keeping 2930000 word types\n",
      "12/07/2021 11:08:24 AM PROGRESS: at sentence #2940000, processed 2940000 words, keeping 2940000 word types\n",
      "12/07/2021 11:08:24 AM PROGRESS: at sentence #2950000, processed 2950000 words, keeping 2950000 word types\n",
      "12/07/2021 11:08:24 AM PROGRESS: at sentence #2960000, processed 2960000 words, keeping 2960000 word types\n",
      "12/07/2021 11:08:24 AM PROGRESS: at sentence #2970000, processed 2970000 words, keeping 2970000 word types\n",
      "12/07/2021 11:08:24 AM PROGRESS: at sentence #2980000, processed 2980000 words, keeping 2980000 word types\n",
      "12/07/2021 11:08:24 AM PROGRESS: at sentence #2990000, processed 2990000 words, keeping 2990000 word types\n",
      "12/07/2021 11:08:24 AM collected 3000000 word types from a corpus of 3000000 raw words and 3000000 sentences\n",
      "12/07/2021 11:08:24 AM Creating a fresh vocabulary\n",
      "12/07/2021 11:08:33 AM Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 3000000 unique words (100.0%% of original 3000000, drops 0)', 'datetime': '2021-12-07T11:08:33.243469', 'gensim': '4.1.2', 'python': '3.6.13 |Anaconda, Inc.| (default, Mar 16 2021, 11:37:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "12/07/2021 11:08:33 AM Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 3000000 word corpus (100.0%% of original 3000000, drops 0)', 'datetime': '2021-12-07T11:08:33.243469', 'gensim': '4.1.2', 'python': '3.6.13 |Anaconda, Inc.| (default, Mar 16 2021, 11:37:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "12/07/2021 11:08:46 AM deleting the raw counts dictionary of 3000000 items\n",
      "12/07/2021 11:08:46 AM sample=0.001 downsamples 0 most-common words\n",
      "12/07/2021 11:08:46 AM Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 3000000 word corpus (100.0%% of prior 3000000)', 'datetime': '2021-12-07T11:08:46.930837', 'gensim': '4.1.2', 'python': '3.6.13 |Anaconda, Inc.| (default, Mar 16 2021, 11:37:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "12/07/2021 11:09:07 AM estimated required memory for 3000000 words and 300 dimensions: 8700000000 bytes\n",
      "12/07/2021 11:09:07 AM resetting layer weights\n",
      "12/07/2021 11:09:11 AM Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2021-12-07T11:09:11.919303', 'gensim': '4.1.2', 'python': '3.6.13 |Anaconda, Inc.| (default, Mar 16 2021, 11:37:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'build_vocab'}\n",
      "12/07/2021 11:09:12 AM Update complete.\n",
      "12/07/2021 11:09:12 AM Loading pretrained embeddings...\n",
      "12/07/2021 11:09:14 AM storing 3000000x300 projection weights into C:\\Users\\bllgu\\AppData\\Local\\Temp\\tmpn0do2ipl\\embeddings.txt\n",
      "12/07/2021 11:16:51 AM loading projection weights from C:\\Users\\bllgu\\AppData\\Local\\Temp\\tmpn0do2ipl\\embeddings.txt\n",
      "12/07/2021 11:27:31 AM KeyedVectors lifecycle event {'msg': 'merged 3000000 vectors into (3000000, 300) matrix from C:\\\\Users\\\\bllgu\\\\AppData\\\\Local\\\\Temp\\\\tmpn0do2ipl\\\\embeddings.txt', 'datetime': '2021-12-07T11:27:31.636830', 'gensim': '4.1.2', 'python': '3.6.13 |Anaconda, Inc.| (default, Mar 16 2021, 11:37:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'intersect_word2vec_format'}\n",
      "12/07/2021 11:27:32 AM Pretrained embeddings loaded to Word2Vec model.\n"
     ]
    }
   ],
   "source": [
    "model.load_embeddings(gensim_model='word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we initialize `Word2Vec` with the pre-trained embeddings, we can do additional training.\n",
    "Here we add the handy `EpochSaver` callback, which saves the model after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/07/2021 11:27:32 AM collecting all words and their counts\n",
      "12/07/2021 11:27:32 AM PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "12/07/2021 11:27:32 AM PROGRESS: at sentence #10000, processed 900659 words, keeping 59826 word types\n",
      "12/07/2021 11:27:32 AM PROGRESS: at sentence #20000, processed 1679397 words, keeping 90519 word types\n",
      "12/07/2021 11:27:33 AM PROGRESS: at sentence #30000, processed 2490190 words, keeping 116946 word types\n",
      "12/07/2021 11:27:33 AM collected 140947 word types from a corpus of 3294344 raw words and 40000 sentences\n",
      "12/07/2021 11:27:33 AM Updating model with new vocabulary\n",
      "12/07/2021 11:27:42 AM Word2Vec lifecycle event {'msg': 'added 111226 new unique words (78.91335040830951%% of original 140947) and increased the count of 29721 pre-existing words (21.086649591690495%% of original 140947)', 'datetime': '2021-12-07T11:27:42.874820', 'gensim': '4.1.2', 'python': '3.6.13 |Anaconda, Inc.| (default, Mar 16 2021, 11:37:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "12/07/2021 11:27:43 AM deleting the raw counts dictionary of 140947 items\n",
      "12/07/2021 11:27:43 AM sample=0.001 downsamples 47 most-common words\n",
      "12/07/2021 11:27:43 AM Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 2509435.463128294 word corpus (76.2%% of prior 3294344)', 'datetime': '2021-12-07T11:27:43.584009', 'gensim': '4.1.2', 'python': '3.6.13 |Anaconda, Inc.| (default, Mar 16 2021, 11:37:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "12/07/2021 11:28:03 AM estimated required memory for 140947 words and 300 dimensions: 408746300 bytes\n",
      "12/07/2021 11:28:03 AM updating layer weights\n",
      "12/07/2021 11:28:09 AM Word2Vec lifecycle event {'update': True, 'trim_rule': 'None', 'datetime': '2021-12-07T11:28:09.357834', 'gensim': '4.1.2', 'python': '3.6.13 |Anaconda, Inc.| (default, Mar 16 2021, 11:37:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'build_vocab'}\n",
      "12/07/2021 11:28:09 AM Word2Vec lifecycle event {'msg': 'training model with 7 workers on 3111226 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=3 shrink_windows=True', 'datetime': '2021-12-07T11:28:09.358834', 'gensim': '4.1.2', 'python': '3.6.13 |Anaconda, Inc.| (default, Mar 16 2021, 11:37:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'train'}\n",
      "12/07/2021 11:28:09 AM Epoch #0 start: 2021-12-07 11:28:09.359834\n",
      "12/07/2021 11:28:10 AM EPOCH 1 - PROGRESS: at 34.88% examples, 922625 words/s, in_qsize 12, out_qsize 0\n",
      "12/07/2021 11:28:11 AM EPOCH 1 - PROGRESS: at 80.30% examples, 1008733 words/s, in_qsize 13, out_qsize 0\n",
      "12/07/2021 11:28:11 AM worker thread finished; awaiting finish of 6 more threads\n",
      "12/07/2021 11:28:11 AM worker thread finished; awaiting finish of 5 more threads\n",
      "12/07/2021 11:28:11 AM worker thread finished; awaiting finish of 4 more threads\n",
      "12/07/2021 11:28:11 AM worker thread finished; awaiting finish of 3 more threads\n",
      "12/07/2021 11:28:11 AM worker thread finished; awaiting finish of 2 more threads\n",
      "12/07/2021 11:28:11 AM worker thread finished; awaiting finish of 1 more threads\n",
      "12/07/2021 11:28:11 AM worker thread finished; awaiting finish of 0 more threads\n",
      "12/07/2021 11:28:11 AM EPOCH - 1 : training on 3294344 raw words (2509992 effective words) took 2.4s, 1028056 effective words/s\n",
      "12/07/2021 11:28:11 AM EPOCH - 1 : supplied example count (40000) did not equal expected count (40001)\n",
      "12/07/2021 11:28:11 AM Word2Vec lifecycle event {'fname_or_handle': 'C:\\\\Users\\\\bllgu\\\\AppData\\\\Local\\\\Temp\\\\tmp5upmt64k\\\\w2v_epoch0.model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2021-12-07T11:28:11.808922', 'gensim': '4.1.2', 'python': '3.6.13 |Anaconda, Inc.| (default, Mar 16 2021, 11:37:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'saving'}\n",
      "12/07/2021 11:28:11 AM storing np array 'vectors' to C:\\Users\\bllgu\\AppData\\Local\\Temp\\tmp5upmt64k\\w2v_epoch0.model.wv.vectors.npy\n",
      "12/07/2021 11:28:28 AM storing np array 'syn1neg' to C:\\Users\\bllgu\\AppData\\Local\\Temp\\tmp5upmt64k\\w2v_epoch0.model.syn1neg.npy\n",
      "12/07/2021 11:28:46 AM not storing attribute cum_table\n",
      "12/07/2021 11:28:47 AM saved C:\\Users\\bllgu\\AppData\\Local\\Temp\\tmp5upmt64k\\w2v_epoch0.model\n",
      "12/07/2021 11:28:47 AM Epoch #0 end: 2021-12-07 11:28:47.878059\n",
      "12/07/2021 11:28:47 AM Elapsed: 0:00:38.518225\n",
      "12/07/2021 11:28:47 AM Word2Vec lifecycle event {'msg': 'training on 3294344 raw words (2509992 effective words) took 38.5s, 65163 effective words/s', 'datetime': '2021-12-07T11:28:47.879060', 'gensim': '4.1.2', 'python': '3.6.13 |Anaconda, Inc.| (default, Mar 16 2021, 11:37:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'train'}\n"
     ]
    }
   ],
   "source": [
    "epoch_saver = emb.EpochSaver('w2v')\n",
    "model.train(corpus, \n",
    "            epochs=1, \n",
    "            callbacks=[epoch_saver])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can save our final model to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/07/2021 11:28:47 AM Word2Vec lifecycle event {'fname_or_handle': 'tuned_gigaword.model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2021-12-07T11:28:47.912551', 'gensim': '4.1.2', 'python': '3.6.13 |Anaconda, Inc.| (default, Mar 16 2021, 11:37:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'saving'}\n",
      "12/07/2021 11:28:47 AM storing np array 'vectors' to tuned_gigaword.model.wv.vectors.npy\n",
      "12/07/2021 11:29:05 AM storing np array 'syn1neg' to tuned_gigaword.model.syn1neg.npy\n",
      "12/07/2021 11:29:23 AM not storing attribute cum_table\n",
      "12/07/2021 11:29:25 AM saved tuned_gigaword.model\n"
     ]
    }
   ],
   "source": [
    "model.model.save('tuned_gigaword.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, we can save just the embeddings. Here they are pickled for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "new_word_vectors = {k: model.model.wv.get_vector(k) for k in model.model.wv.index_to_key}\n",
    "with open('tuned_embeddings_.pkl', 'wb') as handle:\n",
    "    pickle.dump(new_word_vectors, handle)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "19ebe2e2d5ba9030f558b00c834084de3a0f46f0e134827fe109ac2d9a2070be"
  },
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
