{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/07/2021 11:17:04 AM adding document #0 to Dictionary(0 unique tokens: [])\n",
      "12/07/2021 11:17:04 AM built Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...) from 9 documents (total 29 corpus positions)\n",
      "12/07/2021 11:17:04 AM Dictionary lifecycle event {'msg': \"built Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...) from 9 documents (total 29 corpus positions)\", 'datetime': '2021-12-07T11:17:04.854564', 'gensim': '4.1.2', 'python': '3.6.13 |Anaconda, Inc.| (default, Mar 16 2021, 11:37:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sklearn.cluster as cluster\n",
    "import httrees as ht\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`httrees` is a Python module for hierarchical topic modeling, providing several text vectorizers to represent your text data, and a hierarchical clustering algorithm meant to operate over said text representations that utilizes multiple flat clusterers. In that sense the model it implements it is actually a general hierarchical clustering algorithm, though the surrounding utilities are tailored towards text data.\n",
    "\n",
    "This notebook serves as a minimal demonstration of the capabilities of the `httrees` module, and how to use it to perform hierarchical topic modeling on a collection of text documents. We will also discuss implementation details at a high level. The source is available for further perusal on [GitHub](https://github.com/bllguo/CourseProject).\n",
    "\n",
    "We will use a publically available dataset of Amazon product reviews from [Kaggle](https://www.kaggle.com/kashnitsky/flat-hierarchical-tf-idf-logreg-baseline/data) that contains review text, up to three levels of categorization, and other features that are not relevant for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Cat1</th>\n",
       "      <th>Cat2</th>\n",
       "      <th>Cat3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The description and photo on this product need...</td>\n",
       "      <td>grocery gourmet food</td>\n",
       "      <td>meat poultry</td>\n",
       "      <td>jerky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This was a great book!!!! It is well thought t...</td>\n",
       "      <td>toys games</td>\n",
       "      <td>games</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am a first year teacher, teaching 5th grade....</td>\n",
       "      <td>toys games</td>\n",
       "      <td>games</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I got the book at my bookfair at school lookin...</td>\n",
       "      <td>toys games</td>\n",
       "      <td>games</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi! I'm Martine Redman and I created this puzz...</td>\n",
       "      <td>toys games</td>\n",
       "      <td>puzzles</td>\n",
       "      <td>jigsaw puzzles</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text                  Cat1  \\\n",
       "0  The description and photo on this product need...  grocery gourmet food   \n",
       "1  This was a great book!!!! It is well thought t...            toys games   \n",
       "2  I am a first year teacher, teaching 5th grade....            toys games   \n",
       "3  I got the book at my bookfair at school lookin...            toys games   \n",
       "4  Hi! I'm Martine Redman and I created this puzz...            toys games   \n",
       "\n",
       "           Cat2            Cat3  \n",
       "0  meat poultry           jerky  \n",
       "1         games         unknown  \n",
       "2         games         unknown  \n",
       "3         games         unknown  \n",
       "4       puzzles  jigsaw puzzles  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train_40k.csv')\n",
    "df = df[['Text', 'Cat1', 'Cat2', 'Cat3']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text representations: `httrees.vectorizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several vectorizers are available in `httrees.vectorizer`:\n",
    "- `CountVectorizer`: Simple bag-of-words vectorizer.\n",
    "- `TfidfVectorizer`: Term frequency-inverse document frequency vectorizer.\n",
    "- `EmbeddingVectorizer`: Vectorizer that uses a pretrained set of word embeddings to represent text. Documents are represented as the average of the embeddings of their words. Unknown words are either skipped or represented with an `<UNK>` embedding.\n",
    "- `KVVectorizer`: Similar to `EmbeddingVectorizer`, the `KVVectorizer` assumes embeddings are provided as `gensim.models.KeyedVectors` instead of a generic key-value store, which allows it to handle unknown words through `KeyedVectors` methods. For instance, for `FastText` embeddings, unknown words will be vectorized using learned character embeddings.\n",
    "\n",
    "They follow the sklearn API. For example, `TfidfVectorizer` can be used like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = ht.vectorizer.TfidfVectorizer(unk=True, threshold=15)    # words that appear <= 15 times are mapped to UNK\n",
    "vectorizer.fit(df['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9214465 , 0.03934427, 0.21747493, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [1.44499565, 0.04113264, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.66926114, 0.00952545, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [1.58949521, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.93894232, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.predict(df['Text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vocabulary is available in the `vocabulary` attribute of the vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNK 0\n",
      "The 1\n",
      "description 2\n",
      "and 3\n",
      "photo 4\n",
      "on 5\n"
     ]
    }
   ],
   "source": [
    "for word, i in vectorizer.vocabulary.items():\n",
    "    print(word, i)\n",
    "    if i > 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the term frequencies are available in the `term_freqs` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 16182\n",
      "description 268\n",
      "and 94682\n",
      "photo 89\n",
      "on 23067\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for word, cnt in vectorizer.term_freqs.items():\n",
    "    print(word, cnt)\n",
    "    i += 1\n",
    "    if i > 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the count-based vectorizers, `EmbeddingVectorizer` and `KVVectorizer`'s `fit` methods don't do anything. Instead, pretrained word vectors are to be passed in on initialization. `EmbeddingVectorizer` takes a `dict` or any other generic key-value store, and `KVVectorizer` takes a `gensim.models.KeyedVectors` object.\n",
    "\n",
    "The below code downloads a set of pretrained word embeddings from `gensim`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/07/2021 11:17:12 AM loading projection weights from C:\\Users\\bllgu/gensim-data\\word2vec-google-news-300\\word2vec-google-news-300.gz\n",
      "12/07/2021 11:17:51 AM KeyedVectors lifecycle event {'msg': 'loaded (3000000, 300) matrix of type float32 from C:\\\\Users\\\\bllgu/gensim-data\\\\word2vec-google-news-300\\\\word2vec-google-news-300.gz', 'binary': True, 'encoding': 'utf8', 'datetime': '2021-12-07T11:17:51.417799', 'gensim': '4.1.2', 'python': '3.6.13 |Anaconda, Inc.| (default, Mar 16 2021, 11:37:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'load_word2vec_format'}\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "fname = 'word2vec-google-news-300'\n",
    "model = api.load(fname)\n",
    "embeddings = {k: model.get_vector(k) for k in model.index_to_key}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.models.keyedvectors.KeyedVectors"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloaded `gensim` models can be passed directly to a `KVVectorizer`, but we will proceed with an `EmbeddingVectorizer` here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also just load from local disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('glove_wiki_300.pkl', 'rb') as f:\n",
    "    embeddings = pickle.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.33335047e-01,  1.78493317e-01, -1.01416264e-01, -2.18229094e-01,\n",
       "       -1.48588412e-01,  1.13462319e-01, -9.58337719e-02,  7.88709108e-02,\n",
       "        8.27962819e-02, -1.59075406e+00,  1.04738616e-02, -5.22186992e-02,\n",
       "       -5.72863122e-02,  1.00738781e-01,  5.79180455e-02,  6.72442959e-02,\n",
       "       -1.33164410e-01,  4.28178940e-05, -5.71114581e-02, -2.29855500e-01,\n",
       "       -1.50674365e-01,  2.44362183e-01,  7.13207257e-02,  2.52037861e-01,\n",
       "       -2.06734137e-01, -2.66635476e-02,  2.24481912e-03, -4.50084562e-02,\n",
       "       -8.28522278e-02,  6.41427917e-02,  1.46519584e-02,  2.06074228e-01,\n",
       "       -2.16876966e-01,  1.75746591e-01, -6.97974121e-01,  1.47815000e-01,\n",
       "        9.90912728e-02, -1.65078191e-02, -7.06567724e-02, -1.03543996e-02,\n",
       "       -6.87062176e-02, -5.48620958e-02, -7.18452754e-02,  7.64035252e-02,\n",
       "        8.73894112e-02,  4.18272725e-02,  4.66988709e-02,  7.64650906e-02,\n",
       "       -1.70375043e-01,  4.85615024e-02,  3.88974835e-02, -1.07020046e-01,\n",
       "       -5.66541356e-02,  3.78862272e-02, -3.02454542e-02,  1.75645230e-01,\n",
       "        2.62453390e-04,  1.63076680e-01,  4.22487388e-02, -5.96798173e-02,\n",
       "        3.90825084e-02,  6.12920007e-03,  2.34582743e-01, -6.47570460e-02,\n",
       "       -1.32106090e-01, -1.58968774e-01,  4.47306778e-03, -1.74856349e-02,\n",
       "        7.62491851e-02,  1.12797182e-01,  4.35598613e-02,  2.20220140e-01,\n",
       "        5.06213780e-02, -1.21137505e-02, -6.87206341e-02, -9.14643198e-02,\n",
       "        1.78578138e-01,  1.34367046e-01, -2.57351759e-01,  9.39336362e-02,\n",
       "       -2.06601771e-01, -5.94819531e-02,  5.35147725e-02, -7.21490085e-02,\n",
       "        1.48505002e-01,  5.17340296e-02, -2.27397040e-01,  2.08536135e-01,\n",
       "        8.13847022e-03,  5.64116395e-02, -3.05504949e-01,  3.92831236e-02,\n",
       "       -1.44777818e-01, -1.85298545e-01,  4.16535928e-02,  1.67727312e-02,\n",
       "       -3.04664090e-01,  6.83440006e-02,  7.50642589e-03, -3.62446809e-01,\n",
       "        5.05304641e-03,  6.92665452e-02,  2.08309322e-02,  1.84199546e-02,\n",
       "       -1.21640118e-01, -5.62064521e-02,  1.11339366e-01,  8.84222726e-02,\n",
       "       -1.55942869e-01,  1.86070725e-01,  1.16921124e-01, -7.38372713e-03,\n",
       "       -2.71501595e-01, -2.14283997e-01,  3.42084867e-02, -2.13667716e-02,\n",
       "       -3.87752365e-02,  5.83250439e-02,  3.83095238e-02, -2.37276363e-01,\n",
       "       -6.10302869e-02, -4.47492284e-02,  7.21420023e-02,  6.97897704e-02,\n",
       "       -2.29325496e-02,  6.04908278e-04,  1.29365315e-01,  1.28452957e-01,\n",
       "        7.35900913e-02,  9.46308324e-02,  1.13992270e-01,  2.21853557e-01,\n",
       "        2.33303993e-01,  5.34463170e-02, -4.67853478e-02,  6.35422710e-02,\n",
       "        8.66258747e-02, -4.28781667e-03,  3.30883519e-02,  2.66580225e-01,\n",
       "       -5.62492250e-02, -4.52094123e-02, -3.92496993e-02, -3.93631908e-02,\n",
       "       -2.69081456e-01, -1.42225137e-01, -1.32254717e-02,  1.37850954e-01,\n",
       "       -4.76851191e-02,  4.50058439e-02,  2.25458095e-01, -7.25214985e-02,\n",
       "        1.32074566e-01, -7.39363169e-02,  2.78724998e-01, -2.68334990e-02,\n",
       "       -1.64447411e-01,  2.88935647e-02, -1.74214535e-02, -1.26199641e-01,\n",
       "        6.10710913e-02, -6.94315151e-02, -5.41039993e-02, -1.11362727e-01,\n",
       "        1.80726997e-02, -3.73905431e-02, -3.95082289e-02,  1.14122730e-01,\n",
       "       -6.40623286e-02,  1.12640726e-01,  5.95992617e-02,  4.32332300e-02,\n",
       "       -5.05879997e-01,  9.45304103e-02, -4.84783629e-02, -7.97788187e-02,\n",
       "        5.19647266e-02, -1.39563903e-02, -4.85564599e-02,  6.44302302e-02,\n",
       "       -2.57875920e-02,  8.77030924e-02,  4.00494987e-02,  4.01224988e-02,\n",
       "       -2.39780921e-02, -1.16093546e-01, -5.80738627e-02,  2.34917098e-03,\n",
       "        9.71382273e-02,  1.26093499e-01, -2.05273998e-01,  3.15893907e-01,\n",
       "        1.61370148e-02,  3.45665000e-02, -3.89008700e-02, -1.01649758e-01,\n",
       "        5.92363592e-03, -1.67227004e-02, -5.26313941e-03, -7.08035386e-02,\n",
       "        8.16127732e-01, -1.76792518e-01,  5.44893170e-02,  1.56600915e-02,\n",
       "        9.82693190e-02,  6.93982275e-02, -5.26429092e-02,  6.77560928e-02,\n",
       "       -1.00968773e-01, -5.71688049e-02, -6.97345189e-02, -8.03093661e-02,\n",
       "        1.08670773e-01,  8.83195613e-03, -4.31499986e-02,  1.19638911e-01,\n",
       "        1.08622046e-01,  3.33029116e-02,  1.81254113e-02,  5.81298349e-02,\n",
       "        7.50590905e-02,  1.22959564e-02, -1.96260366e-01,  1.77586814e-02,\n",
       "        5.03126258e-02,  1.53676359e-02,  1.59673357e-01,  1.97144044e-01,\n",
       "        1.08606357e-02, -5.96220744e-02, -9.92759089e-03,  1.09049875e-01,\n",
       "       -7.08649518e-02, -8.48887295e-02,  1.37262417e-01,  6.54778623e-02,\n",
       "        7.68148615e-02, -6.42809373e-02, -1.72014101e-02,  2.07080820e-01,\n",
       "       -4.79745017e-02,  9.44407751e-02,  2.77804408e-01,  5.91379998e-02,\n",
       "       -6.89815004e-01, -4.06916817e-02,  8.86959075e-02, -1.07629410e-01,\n",
       "        1.68036822e-02, -7.46926704e-02,  4.73902186e-02, -1.16860729e-01,\n",
       "       -3.01266425e-02, -2.86615913e-02,  1.52213534e-01,  1.33023188e-02,\n",
       "       -7.07685474e-02,  6.61221148e-02,  1.59566865e-01,  9.66009758e-02,\n",
       "       -1.05672279e-02, -6.12999912e-02,  8.05503198e-02,  1.41254177e-02,\n",
       "       -1.12301047e-01, -1.64400009e-02, -1.78582515e-02, -1.53964746e-02,\n",
       "        2.05267506e-01, -1.08554319e-01, -5.46356912e-02, -9.80295452e-02,\n",
       "        4.06729802e-02,  5.16409488e-02, -5.10974994e-02,  1.71700224e-01,\n",
       "       -1.72689456e+00, -4.73783986e-02,  5.20726087e-01,  9.93252265e-02,\n",
       "       -1.37528908e-01, -5.98819080e-02, -3.67940916e-02,  4.89950368e-02,\n",
       "       -1.03224991e-02,  8.63972730e-02, -9.11181354e-02,  3.15108851e-03,\n",
       "       -5.94562362e-02,  3.90345449e-02,  5.08667124e-02, -1.96007954e-01,\n",
       "        6.91925887e-02,  5.97989983e-02,  5.32772759e-02,  1.61348815e-01,\n",
       "       -3.05260155e-03, -1.94197173e-01, -5.96734063e-02, -7.19830453e-02])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_vectorizer = ht.vectorizer.EmbeddingVectorizer(embeddings)\n",
    "X = e_vectorizer.predict(df['Text'])\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also fine-tune our embeddings on our data, adding new words to the vocabulary, and updating the embeddings of existing words. This process is detailed in [a separate notebook here](https://github.com/bllguo/CourseProject/blob/main/example_finetune.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the model: `httrees.models`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have transformed our data with a vectorizer, we can use the `httrees.models` module to run a hierarchical clustering algorithm. \n",
    "\n",
    "The `TopicTreeModel` builds a tree representation of the topic hierarchy. \n",
    "\n",
    "First, we describe the tree itself. In this tree, every node is a topic/cluster. A node can have any number of children, each of which represent a subtopic underneath it. A `Node` has the following attributes:\n",
    "- depth: The depth of the node in the tree. This is important for the algorithm as the depth determines which clustering model to use.\n",
    "- mask: Every node stores a boolean mask that indicates which documents in the training data are assigned to it. A child node's mask will be a subset of the parent's mask.\n",
    "- center: Once the model has been fit, the center of the node represents the cluster center, typically meaning an average of the assigned documents.\n",
    "- children: A list of child nodes.\n",
    "\n",
    "The `TopicTreeModel` builds this tree according to a given configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = [(cluster.MiniBatchKMeans, {'validate': {'n_clusters': {'values': [8, 10]}}}),\n",
    "          (cluster.MeanShift, {}),\n",
    "          (cluster.AgglomerativeClustering, \n",
    "           {'n_clusters': 3,\n",
    "            'affinity': 'euclidean', \n",
    "            'linkage': 'ward'})\n",
    "          ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 elements in this example config, meaning that the hierarchy will be 3 levels deep. If only 2 elements are in the list, the hierarchy will be 2 levels deep, and so on. Each element contains a clustering model, like `cluster.MiniBatchKMeans`, and a dictionary containing the parameters for that clustering model. See [sklearn documentation](https://scikit-learn.org/stable/modules/clustering.html#) for some other compatible models. `httrees` was tested on kmeans, affinity propagation, mean shift, and agglomerative clustering.\n",
    "\n",
    "The `validate` parameter, seen above as an input for `MiniBatchKMeans`, is not a native sklearn param. It is a dict of model params that the `TopicTreeModel` will evaluate to find the best choice. This allows for usage of clustering models where `n_clusters` must be specified, for example. In the above config, 8 and 10 clusters will be tested for the highest level category. Details on how this validation is performed is located [in the source](https://github.com/bllguo/CourseProject/blob/main/httrees/models.py#L82-L109). BE VERY CAREFUL when using this parameter, as the validation computation is very costly (it computes pairwise cosine similarities between all documents in a cluster).\n",
    "\n",
    "For every node at a given depth in the tree, one instance of the model corresponding to that depth will be fit. For instance, in this case, `MiniBatchKMeans` will be fit at the first level to generate the highest-level topics. This will be saved in the root node. In this example, we might generate very broad topics/clusters corresponding to Amazon categories like 'Electronics', 'Home & Garden', and 'Toys & Games'.\n",
    "\n",
    "These clusters will be child nodes of the root node. For every cluster, a `MeanShift` model will be fit to generate 3rd level subtopics. For instance, the `MeanShift` model for the 'Electronics' cluster could generate 3 subtopics corresponding to 'Cell Phones & Accessories', 'Computers', and 'Tablets'. These will be child nodes of the 'Electronics' node. A separate `MeanShift` model will be fit for `Home & Garden`, etc.\n",
    "\n",
    "This continues until the max depth specified by the config - in this case, 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ht.models.TopicTreeModel(config)\n",
    "tree.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `TopicTreeModel` provides a `decode` method that returns the cluster assignments as an array of shape (`X.shape[0]`, `len(config)`) - a row for every document - as well as a summary of each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   3,  733, 3404],\n",
       "       [   1,    9, 2680],\n",
       "       [   6, 1537, 4208],\n",
       "       ...,\n",
       "       [   2,  693, 3364],\n",
       "       [   4, 1208, 3879],\n",
       "       [   7, 2023, 4694]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assignments, clusters = tree.decode()\n",
    "assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topics': [],\n",
       " 'center': array([-1.05563528e-01,  9.60578198e-02, -6.02419291e-02, -1.47723918e-01,\n",
       "        -4.84223794e-02, -1.32323935e-03, -4.77894796e-02,  3.68519057e-02,\n",
       "         1.08842833e-01, -1.56762253e+00,  1.05297648e-01, -2.96364271e-02,\n",
       "        -6.62311710e-02,  1.14088915e-01,  1.26955661e-02,  8.62555348e-02,\n",
       "        -1.54350493e-01,  4.17050668e-03, -6.45460731e-03, -1.99513326e-02,\n",
       "         2.74512330e-02,  2.46578148e-01,  1.05359679e-01,  1.08188672e-01,\n",
       "        -2.49874049e-01, -6.18180829e-02,  3.47082471e-02, -1.34118283e-01,\n",
       "        -5.09657673e-02,  5.69302300e-02, -1.61627532e-02,  2.53710563e-01,\n",
       "        -2.28054691e-01, -1.61259225e-02, -8.50624737e-01,  1.69397810e-01,\n",
       "        -8.06885621e-02,  2.08911511e-02, -6.97873893e-02,  4.03181623e-02,\n",
       "         2.03055585e-02, -1.42359836e-01, -4.31400195e-02, -5.23352777e-02,\n",
       "         1.28550935e-01,  9.26878994e-02,  2.07890434e-01,  1.30938008e-01,\n",
       "        -6.26850417e-02,  5.77091664e-02,  7.57689740e-02, -1.30101359e-01,\n",
       "         7.83067856e-02, -6.18466926e-02, -1.50970331e-01,  1.70618234e-01,\n",
       "        -1.01227311e-02,  1.35268300e-01,  1.85157874e-01,  9.26439145e-03,\n",
       "         1.80625924e-01, -2.82928470e-03,  1.87112928e-01,  1.17611263e-01,\n",
       "        -6.54596995e-02, -3.13602299e-01,  4.40574744e-02,  4.17837589e-02,\n",
       "         7.82785167e-02, -7.37690551e-03,  6.01830513e-02, -2.23017231e-02,\n",
       "         1.64423682e-02,  1.74495413e-01, -1.05114782e-02, -4.48300412e-02,\n",
       "         9.69342328e-02,  9.02140869e-02, -1.56026137e-01, -6.64974701e-02,\n",
       "        -1.48431704e-01, -5.07149275e-04,  2.20937178e-01, -3.90471064e-02,\n",
       "         9.08675717e-02,  1.10032787e-01, -1.16211437e-01,  1.88512951e-01,\n",
       "        -4.38934793e-03,  1.08589918e-01, -1.84442537e-01,  1.69454964e-01,\n",
       "        -1.72331202e-01, -1.76488735e-01, -3.37404377e-02,  2.22925469e-02,\n",
       "        -2.30116149e-01, -2.21829463e-02,  8.79316438e-02, -2.34273480e-01,\n",
       "         6.01318343e-02, -2.43892639e-02, -9.86826001e-02, -9.15737434e-02,\n",
       "        -8.34852038e-02, -1.02820912e-02,  1.35375531e-01,  7.91602206e-02,\n",
       "        -2.07332443e-01,  1.10411927e-01, -2.70680323e-02, -9.50753014e-02,\n",
       "        -1.19666988e-01, -2.31976364e-01,  9.92038599e-02,  1.43037945e-01,\n",
       "        -1.34766366e-01,  9.02293385e-02,  3.54933114e-02, -2.06064296e-01,\n",
       "        -9.22472254e-02, -1.47127618e-01,  7.26210543e-02,  1.55455087e-01,\n",
       "        -1.51315945e-02, -7.88286740e-02,  6.92488907e-02,  1.62400185e-01,\n",
       "         8.26802014e-02,  6.88316373e-02,  1.69145129e-01,  1.01154507e-01,\n",
       "         9.37702628e-02,  4.80352594e-02, -9.89745089e-02,  2.64869823e-02,\n",
       "        -2.13606055e-02, -3.01997250e-02, -1.06134779e-04,  1.68714407e-01,\n",
       "        -1.97461930e-04,  1.18943213e-01,  3.35268385e-02,  8.60340484e-02,\n",
       "        -3.08911140e-01, -7.09707959e-02,  3.37925858e-03,  4.96141074e-02,\n",
       "        -8.28001719e-02,  4.25635141e-02, -5.09347131e-04, -3.57985551e-02,\n",
       "         3.14615541e-02, -5.78362419e-02,  2.04460288e-01,  2.84459887e-03,\n",
       "        -2.74260711e-02, -8.58136722e-02,  1.23210357e-01, -3.73458989e-03,\n",
       "         8.72088703e-02, -2.06640513e-01, -3.24942730e-03, -5.67036770e-02,\n",
       "         5.73274314e-03,  8.28991003e-02, -8.67634476e-03,  1.66654083e-01,\n",
       "         9.37654271e-02,  5.32094587e-02,  2.20402084e-02,  7.46834050e-02,\n",
       "        -4.87372294e-01,  2.57106307e-02, -1.82040692e-02, -5.70062164e-02,\n",
       "        -6.10257045e-02,  6.84877315e-02,  1.38338000e-02,  1.34423899e-01,\n",
       "         9.84393106e-02,  3.32069328e-02,  1.60545103e-01, -1.66794695e-03,\n",
       "        -3.43850914e-02, -7.03568209e-02,  6.94936859e-02,  8.76259571e-02,\n",
       "         6.95161346e-02,  7.11499120e-02, -5.86028518e-02,  1.35623534e-01,\n",
       "         3.79998573e-02, -5.12939515e-02, -1.41798725e-02, -1.42072789e-01,\n",
       "        -6.44265428e-02, -2.48826468e-03,  4.07797882e-02, -6.89356182e-02,\n",
       "         9.62237673e-01, -4.92537404e-02,  1.02113203e-01,  5.49487236e-02,\n",
       "         7.27196527e-02,  6.26803569e-02, -1.98550024e-02,  9.87187543e-02,\n",
       "        -1.19224258e-01, -1.31203490e-01, -9.99703451e-02, -5.20945358e-02,\n",
       "         8.09544443e-02,  2.90327974e-02,  3.91185804e-02,  5.74652208e-02,\n",
       "         1.10610027e-01,  1.87806803e-03,  1.61399253e-02, -1.91590617e-02,\n",
       "         1.63720870e-01, -5.39013843e-02, -1.82480709e-01,  6.58826660e-02,\n",
       "         2.89276372e-02,  3.39926536e-02,  1.06827527e-01,  5.56416337e-02,\n",
       "        -1.97373197e-02, -6.03723596e-02,  5.69182497e-02,  1.83230181e-02,\n",
       "         1.07922826e-02, -1.79107953e-01,  4.12961023e-02,  8.56315202e-02,\n",
       "         3.75317338e-02,  3.45714224e-02, -1.45326066e-01,  2.66286594e-02,\n",
       "         3.59469683e-02,  3.38627923e-02,  1.92735704e-01,  9.42937365e-02,\n",
       "        -5.57818716e-01, -4.94836219e-02,  1.39388076e-01,  2.24514429e-02,\n",
       "        -3.50180982e-02, -1.15012587e-01,  5.71635002e-02, -1.42693165e-01,\n",
       "        -1.61963799e-02, -2.11698158e-02,  3.20100893e-01,  5.87842971e-02,\n",
       "        -1.83772220e-02, -1.07419751e-01,  6.82932906e-02,  1.18916386e-02,\n",
       "        -1.89572767e-02, -1.42003194e-01,  2.58863694e-02,  2.81150492e-02,\n",
       "        -5.92314889e-02,  4.40943149e-02, -8.78544319e-02, -5.98582635e-02,\n",
       "         9.21792014e-02,  1.33078515e-01,  3.21832790e-03, -1.27931165e-01,\n",
       "         5.50732297e-02,  8.55620110e-02, -1.68291203e-03,  6.33600299e-02,\n",
       "        -1.76926367e+00, -5.29317621e-02,  2.14818501e-01,  3.97027562e-02,\n",
       "        -1.61852125e-01,  5.00025428e-02,  2.93091482e-02,  5.20111935e-02,\n",
       "        -8.65804228e-02,  1.91388281e-01, -1.90448949e-02,  8.76287037e-02,\n",
       "        -3.37967615e-02, -2.54679506e-02, -2.92243226e-02, -1.77669184e-01,\n",
       "        -9.13137240e-03,  2.92496548e-02,  9.57555729e-02, -5.26896641e-02,\n",
       "         8.79684551e-02, -1.68158939e-01, -4.76917812e-02,  1.06100397e-01]),\n",
       " 'mask': array([False, False, False, ..., False, False, False])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If provided the original texts and vectorizer, `decode` will also attempt to do some summarization and associate each `cluster` with a topic string. This is very experimental and doesn't handle noise words well - it is based on processing the most common bigrams in each cluster - so it will perform poorly on longer documents.\n",
    "\n",
    "This can be turned into a dataframe through the `flatten` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignments, clusters = tree.decode(df['Text'], vectorizer=e_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>level1</th>\n",
       "      <th>level2</th>\n",
       "      <th>level3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The description and photo on this product need...</td>\n",
       "      <td>[to be, to the, and the]</td>\n",
       "      <td>[to be, to the, and the]</td>\n",
       "      <td>[to be, to the, and the]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This was a great book!!!! It is well thought t...</td>\n",
       "      <td>[This is, it is, It is]</td>\n",
       "      <td>[It is, it is, and it]</td>\n",
       "      <td>[It is, it is, and it]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am a first year teacher, teaching 5th grade....</td>\n",
       "      <td>[to the, and the, and I]</td>\n",
       "      <td>[to the, and the, and I]</td>\n",
       "      <td>[to the, and the, and I]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I got the book at my bookfair at school lookin...</td>\n",
       "      <td>[and I, and it, I would]</td>\n",
       "      <td>[and I, and it, I would]</td>\n",
       "      <td>[and I, and it, I would]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi! I'm Martine Redman and I created this puzz...</td>\n",
       "      <td>[I have, is the, and the]</td>\n",
       "      <td>[I have, is the, and the]</td>\n",
       "      <td>[I have, is the, and the]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>My eight year old loves this game, whenever he...</td>\n",
       "      <td>[this for, for my, and I]</td>\n",
       "      <td>[this for, for my, and I]</td>\n",
       "      <td>[this for, for my, and I]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The real joy of this movie doesn't lie in its ...</td>\n",
       "      <td>[to be, to the, and the]</td>\n",
       "      <td>[to be, to the, and the]</td>\n",
       "      <td>[to be, to the, and the]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Okay, Tim Burton is genuine. He haunts you wit...</td>\n",
       "      <td>[This is, it is, It is]</td>\n",
       "      <td>[It is, it is, and it]</td>\n",
       "      <td>[It is, it is, and it]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Boundaries, along with counseling, has given m...</td>\n",
       "      <td>[This is, it is, It is]</td>\n",
       "      <td>[It is, it is, and it]</td>\n",
       "      <td>[It is, it is, and it]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>120 colors? I say 120 sticks of fun! And a fre...</td>\n",
       "      <td>[This is, it is, It is]</td>\n",
       "      <td>[It is, it is, and it]</td>\n",
       "      <td>[It is, it is, and it]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  The description and photo on this product need...   \n",
       "1  This was a great book!!!! It is well thought t...   \n",
       "2  I am a first year teacher, teaching 5th grade....   \n",
       "3  I got the book at my bookfair at school lookin...   \n",
       "4  Hi! I'm Martine Redman and I created this puzz...   \n",
       "5  My eight year old loves this game, whenever he...   \n",
       "6  The real joy of this movie doesn't lie in its ...   \n",
       "7  Okay, Tim Burton is genuine. He haunts you wit...   \n",
       "8  Boundaries, along with counseling, has given m...   \n",
       "9  120 colors? I say 120 sticks of fun! And a fre...   \n",
       "\n",
       "                      level1                     level2  \\\n",
       "0   [to be, to the, and the]   [to be, to the, and the]   \n",
       "1    [This is, it is, It is]     [It is, it is, and it]   \n",
       "2   [to the, and the, and I]   [to the, and the, and I]   \n",
       "3   [and I, and it, I would]   [and I, and it, I would]   \n",
       "4  [I have, is the, and the]  [I have, is the, and the]   \n",
       "5  [this for, for my, and I]  [this for, for my, and I]   \n",
       "6   [to be, to the, and the]   [to be, to the, and the]   \n",
       "7    [This is, it is, It is]     [It is, it is, and it]   \n",
       "8    [This is, it is, It is]     [It is, it is, and it]   \n",
       "9    [This is, it is, It is]     [It is, it is, and it]   \n",
       "\n",
       "                      level3  \n",
       "0   [to be, to the, and the]  \n",
       "1     [It is, it is, and it]  \n",
       "2   [to the, and the, and I]  \n",
       "3   [and I, and it, I would]  \n",
       "4  [I have, is the, and the]  \n",
       "5  [this for, for my, and I]  \n",
       "6   [to be, to the, and the]  \n",
       "7     [It is, it is, and it]  \n",
       "8     [It is, it is, and it]  \n",
       "9     [It is, it is, and it]  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = tree.flatten(df['Text'], assignments, clusters)\n",
    "out.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "19ebe2e2d5ba9030f558b00c834084de3a0f46f0e134827fe109ac2d9a2070be"
  },
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
